name: ai-load-balancer

services:
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --save 60 1 --loglevel warning

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana:/etc/grafana/provisioning
    depends_on:
      - prometheus

  service:
    build: .
    container_name: ai-load-balancer
    ports:
      - "8080:8080"
    environment:
      - OPENAI_URL=https://api.openai.com/v1/chat/completions
      - OPENAI_KEY=
      - OPENAI_TIMEOUT=1m
      - OPENAI_RPM=60
      - HUGGINGFACE_URL=https://api-inference.huggingface.co/models/
      - HUGGINGFACE_KEY=
      - HUGGINGFACE_TIMEOUT=1m
      - HUGGINGFACE_RPM=60
      - OPENROUTER_URL=https://openrouter.ai/api/v1/chat/completions
      - OPENROUTER_KEY=
      - OPENROUTER_TIMEOUT=1m
      - OPENROUTER_RPM=60
      - REDIS_URL=redis:6379
      - REDIS_PASSWORD=
      - USER_RPM=100
      - MODEL_TO_CLIENTS=model-to-clients.json
      - CACHE_RESPONSE_EXPIRATION_TIME=5m
    depends_on:
      - redis
      - grafana
